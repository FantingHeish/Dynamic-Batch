# Dynamic Batch

## ðŸŽ¯ å°ˆæ¡ˆç°¡ä»‹
æœ¬å°ˆæ¡ˆé‡å° LLM æŽ¨è«–çš„æ•ˆèƒ½ç“¶é ¸ï¼ˆGPU idleã€prefill é‡è¤‡è¨ˆç®—ã€decode ä½Žæ•ˆçŽ‡ï¼‰ï¼Œæ‰“é€ ä¸€å€‹ ç ”ç©¶ç”¨ Dynamic Batch Inference Engineï¼ŒåŠŸèƒ½åŒ…å«ï¼š
- Dynamic batchingï¼ˆåˆä½µå¤šå€‹ requestï¼‰
- Shared prefillï¼ˆä¸€æ¬¡å‰å‘è¨ˆç®—å¤šåºåˆ—ï¼‰
- KV-cache reuseï¼ˆæ¸›å°‘ attention è¨ˆç®—ï¼‰
- Dynamic padding + attention masking
- Per-request TTFT / Latency profiling
- GPU utilization & memory profilingï¼ˆNVMLï¼‰
æ¸¬è©¦æ¨¡åž‹ï¼šQwen2.5-0.5B-Instruct
æ¸¬è©¦ç¡¬é«”ï¼šNVIDIA Tesla T4ï¼ˆ15GBï¼‰

âœ…
## ðŸš€ æŠ€è¡“æ ¸å¿ƒ
### ðŸ”¸ 1. Batch Prefillï¼ˆä¸€æ¬¡å‰å‘è¨ˆç®—æ‰€æœ‰åºåˆ—ï¼‰
#### ðŸŽ¯ ä½œæ³•ï¼š
- è¼¸å…¥å¤šå€‹ prompt â†’ dynamic padding â†’ attention_mask â†’ å–®æ¬¡å‰å‘ï¼š
  - å°‡å¤šå€‹ prompt åš dynamic padding â†’ attention_mask
  - ä½¿ç”¨å–®æ¬¡ model.forward åŸ·è¡Œ prefill
  - ç”¢ç”Ÿ past_key_valuesï¼ˆKV-cacheï¼‰
  - æŠ½å‡ºæ¯å€‹åºåˆ—æœ€å¾Œ token ä½œç‚º decode èµ·é»ž
#### ðŸ‘‰ é”æˆï¼š
- âœ” å¤§å¹…é™ä½Žé‡è¤‡è¨ˆç®—
- âœ” prefill è¨ˆç®—é‡ç”± N æ¬¡ â†’ 1 æ¬¡
- âœ” å»ºç«‹ batch decode çš„åŸºç¤Ž

### ðŸ”¸ 2. Batch Autoregressive Decodeï¼ˆé€ token æ‰¹æ¬¡ decodeï¼‰
#### ðŸŽ¯ ä½œæ³•ï¼š
1. Decode loopï¼šï¼ˆæ¯å€‹ decode stepï¼‰
- å…±ç”¨ KV-cacheï¼ˆå¤§å¹…æ¸›å°‘çŸ©é™£ä¹˜æ³•ï¼‰
- multi-head attention åªéœ€è™•ç†æ–°å¢žä½ç½®
- batch è¶Šå¤§ â†’ CUDA kernel è¶Šé£½å’Œ â†’ GPU åˆ©ç”¨çŽ‡æ›´é«˜
#### ðŸ‘‰ é”æˆï¼š
- âœ” throughput æˆé•·
- âœ” decode latency ä¸‹é™

### ðŸ”¸ 3. Dynamic Padding + Attention Masking
#### ðŸŽ¯ ä½œæ³•ï¼š
- ç‚º batch ä¸­è¼ƒçŸ­çš„åºåˆ—è‡ªå‹• padding
- ä»¥ attention_mask ç¢ºä¿æ¨¡åž‹å¿½ç•¥ pad token
- ä¿æŒ batch è¨ˆç®—ä¸€è‡´æ€§
#### ðŸ‘‰ é”æˆï¼š
- âœ” ä¸æµªè²»è¨ˆç®—åœ¨ pad token
- âœ” æ”¯æ´ä¸åŒé•·åº¦åºåˆ—
- âœ” æœ¬è³ªç‚º batch-level paddingï¼Œä¸ç­‰åŒæ–¼ PagedAttentionï¼ˆä½†æ¦‚å¿µä¸ŠåŒæ¨£æ˜¯æ¸›å°‘ä¸å¿…è¦çš„è¨ˆç®—ï¼‰

### ðŸ”¸ 4. Per-request Metrics Profiling
| æŒ‡æ¨™                        | ç”¨é€”                          |
| ------------------------- | --------------------------- |
| TTFTï¼ˆTime to First Tokenï¼‰ | æ¸¬é‡ decode ç¬¬ä¸€å€‹ token çš„é€Ÿåº¦     |
| prefill_ms                | å‰å‘ä¸€æ¬¡å¤šåºåˆ—èŠ±è²»æ™‚é–“                 |
| decode_ms                 | autoregressive å…¨éƒ¨ decode æ™‚é–“ |
| latency_ms                | å–®å€‹ request çš„ end-to-end æ™‚é–“  |
| P50/P95 latency           | è¡¡é‡ tail latencyï¼Œç”Ÿç”¢ç³»çµ±é—œéµæŒ‡æ¨™    |
| throughput (tokens/sec)   | GPU çš„æ•´é«”æŽ¨è«–æ•ˆçŽ‡                   |


## ðŸ§° æŠ€è¡“æž¶æ§‹
| æ¨¡çµ„              | æŠ€è¡“                                                |
| --------------- | ------------------------------------------------- |
| æ·±åº¦å­¸ç¿’æ¡†æž¶          | PyTorch 2.0+ã€CUDA FP16                            |
| æŽ¨è«–å¼•æ“Žæ ¸å¿ƒ          | Batch Prefillã€Batch Decodeã€KV-cache Reuse         |
| å¼µé‡è™•ç†            | Dynamic Paddingã€Attention Masking                 |
| æ¨¡åž‹              | HuggingFace Transformers (`use_cache=True`)       |
| Batch Engine è¨­è¨ˆ | Static Baseline vs Fixed-size Dynamic Batch Fill  |
| æ•ˆèƒ½ç›£æ¸¬            | TTFTã€Prefill/Decode æ™‚é–“ã€Latency P50/P95ã€Throughput |
| GPU Profiling   | NVMLï¼šGPU Utilization / Memory Tracking            |
| å·¥ä½œè² è¼‰            | æ¨¡æ“¬ 32 å€‹ requestã€æ¯æ¬¡ decode 64 tokens               |
| çµ±è¨ˆ              | pandas / numpyï¼ˆè¼¸å‡º summary tableï¼‰                  |



## ðŸ“Š æ•ˆèƒ½æŒ‡æ¨™
### ðŸ“Š 1. Benchmark çµæžœ
æœ¬æ¬¡æ¸¬è©¦å…± 32 å€‹è«‹æ±‚ã€æ¯æ¬¡ decode 64 tokensï¼Œå°æ¯”ï¼š
- static_single (batch=1) â†’ baseline
- dynamic_bs=2 / 4 / 8 â†’ æ¨¡æ“¬ dynamic batching è¡Œç‚º
- bs=8 å° T4) æ˜¯æœ€ä½³ trade-offï¼Œé¡¯ç¤º dynamic batching çš„æ•ˆæžœèˆ‡ SM/Memory çµæ§‹æœ‰é—œã€‚
| Mode              | Throughput (tokens/s) | Speedup   | P95 Latency | GPU Util Avg | GPU Util Max | Max Mem |
| ----------------- | --------------------- | --------- | ----------- | ------------ | ------------ | ------- |
| **static_single** | **248**               | 1.00Ã—     | **7994 ms** | 21%          | 46%          | 2.46 GB |
| **dynamic_bs=2**  | 302                   | 1.21Ã—     | 4450 ms     | 36%          | 52%          | 2.46 GB |
| **dynamic_bs=4**  | 791                   | 3.19Ã—     | 2588 ms     | 37%          | 45%          | 2.48 GB |
| **dynamic_bs=8**  | **869**               | **3.50Ã—** | **2356 ms** | **37%**      | 43%          | 2.53 GB |


#### ðŸŽ¯ 2. Key Findings
##### â­ 1. Throughput ä¸Šå‡ï¼š3.5Ã—
Dynamic batching è®“ GPU å¾—ä»¥ä¸€æ¬¡è™•ç†æ›´å¤šåºåˆ—
##### â­ 2. Latency é¡¯è‘—ä¸‹é™ï¼šâˆ’70.5%
å¤šå€‹è«‹æ±‚ å…±ç”¨ä¸€æ¬¡ Prefillï¼Œå¤§å¹…æ”¤å¹³ Self-Attention çš„å›ºå®šæˆæœ¬ã€‚Decode éŽç¨‹ä¹Ÿå› å¤šå€‹åºåˆ—ä½µå…¥åŒä¸€ kernel è€Œåžåæå‡ã€‚
##### â­ 3. GPU ä½¿ç”¨çŽ‡æå‡ +15%
Baseline GPU idle æ˜Žé¡¯ï¼ˆ21%ï¼‰ã€‚Dynamic batching å¾Œ GPU åˆ©ç”¨çŽ‡æå‡åˆ° 36â€“38%ï¼š
##### â­ 4. GPU Memory å¹¾ä¹Žä¸è®Šï¼ˆ+3%ï¼‰
Dynamic padding + KV-cache reuse æˆåŠŸæŽ§åˆ¶è¨˜æ†¶é«”ã€‚2.46GB â†’ 2.53GBï¼ˆ+2.9%ï¼‰ã€‚
##### ðŸŽ‰ bs=8 åœ¨ T4 GPU ä¸Šæ˜¯æœ€ä½³ sweet spotï¼ˆæ•ˆèƒ½ â†’ è¨˜æ†¶é«”çš„æœ€ä½³trade-offï¼‰

## ç’°å¢ƒéœ€æ±‚
ðŸ–¥ï¸ ç’°å¢ƒéœ€æ±‚
- Python 3.9+
- CUDA 11.8+
- PyTorchï¼ˆæ”¯æ´ FP16ï¼‰
- transformers >= 4.44
- NVIDIA GPUï¼ˆâ‰¥ 6GB VRAMï¼‰
- transformers >= 4.44
- GPU â‰¥ 6GB VRAM
