# -*- coding: utf-8 -*-
"""PoC_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EiJi1InAmPkKfA4WY4G9xfv307SDD4Zp
"""

# ============================================================
# PoC#2: Throughput & Batch Size å¯¦é©—ï¼ˆä¿®æ­£ç‰ˆï¼‰
#
# ç›®æ¨™ï¼šæ¸¬é‡ä¸åŒ Batch Size å°ååé‡çš„å½±éŸ¿
#
# æ ¸å¿ƒå•é¡Œï¼š
# 1. Batch Size å¦‚ä½•å½±éŸ¿ Throughputï¼ˆtokens/secï¼‰ï¼Ÿ
# 2. Batch Size å¦‚ä½•å½±éŸ¿ Latencyï¼ˆmsï¼‰ï¼Ÿ
# 3. GPU åˆ©ç”¨ç‡çš„æ¬Šè¡¡é»åœ¨å“ªï¼Ÿ
# ============================================================

print("="*60)
print("PoC#2: Throughput & Batch Size å¯¦é©—")
print("="*60)

# ============================================================
# PART 1: ç’°å¢ƒè¨­å®š
# ============================================================

print("\nğŸ“¦ å®‰è£ä¾è³´...")
import subprocess
import sys

# å®‰è£å¿…è¦å¥—ä»¶
subprocess.check_call([
    sys.executable, "-m", "pip", "install", "-q",
    "transformers>=4.44.0", "accelerate>=0.33.0",
    "torch", "--index-url", "https://download.pytorch.org/whl/cu121"
])
subprocess.check_call([
    sys.executable, "-m", "pip", "install", "-q",
    "fastapi", "uvicorn", "pydantic", "requests",
    "pandas", "matplotlib", "seaborn"
])

print("âœ… å®‰è£å®Œæˆ")

# æª¢æŸ¥ GPU
import torch
if torch.cuda.is_available():
    print(f"\nâœ… GPU å¯ç”¨: {torch.cuda.get_device_name(0)}")
    print(f"   è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    print("\nâš ï¸  æœªåµæ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPUï¼ˆé€Ÿåº¦è¼ƒæ…¢ï¼‰")

# ============================================================
# PART 2: Server å¯¦ä½œï¼ˆç°¡åŒ–ç‰ˆï¼‰
# ============================================================

server_code = '''
"""
ç°¡åŒ–ç‰ˆ Batch Inference Server

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. /generate_batch: æ¥å—å¤šå€‹è«‹æ±‚ï¼Œæ‰¹æ¬¡è™•ç†
2. /generate_single: å–®ä¸€è«‹æ±‚è™•ç†ï¼ˆå°ç…§çµ„ï¼‰
3. /health: å¥åº·æª¢æŸ¥

æŠ€è¡“é‡é»ï¼š
- ä½¿ç”¨ HuggingFace generate() çš„ batch èƒ½åŠ›
- å‹•æ…‹ padding è™•ç†ä¸åŒé•·åº¦è¼¸å…¥
- è¨˜éŒ„è©³ç´°çš„æ•ˆèƒ½æŒ‡æ¨™
"""

import time
import uuid
from typing import List
import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import asyncio

# ===== é…ç½® =====
MODEL_ID = "Qwen/Qwen2-0.5B-Instruct"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.float16 if DEVICE == "cuda" else torch.float32

app = FastAPI()

print(f"ğŸš€ è¼‰å…¥æ¨¡å‹: {MODEL_ID}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=DTYPE,
    device_map="auto"
).eval()

print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ (Device: {DEVICE})")

# ===== è³‡æ–™çµæ§‹ =====
class GenRequest(BaseModel):
    prompt: str
    max_new_tokens: int = 64
    temperature: float = 0.7

class BatchRequest(BaseModel):
    prompts: List[str]
    max_new_tokens: int = 64
    temperature: float = 0.7

# ===== æ‰¹æ¬¡ç”Ÿæˆï¼ˆæ ¸å¿ƒå‡½æ•¸ï¼‰=====
@torch.no_grad()
def batch_generate(prompts: List[str], max_new_tokens: int, temperature: float):
    """
    æ‰¹æ¬¡ç”Ÿæˆå‡½æ•¸

    é—œéµæŠ€è¡“ï¼š
    1. å‹•æ…‹ paddingï¼šè™•ç†ä¸åŒé•·åº¦çš„è¼¸å…¥
    2. attention_maskï¼šæ¨™è¨˜æœ‰æ•ˆ tokens
    3. æ‰¹æ¬¡æ¨ç†ï¼šä¸€æ¬¡å‰å‚³è™•ç†å¤šå€‹åºåˆ—

    æ™‚é–“è¤‡é›œåº¦ï¼šO(batch_size * seq_len^2) [attention]
    ç©ºé–“è¤‡é›œåº¦ï¼šO(batch_size * seq_len * hidden_dim)
    """
    t_start = time.time()

    # Tokenize with padding
    inputs = tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    ).to(DEVICE)

    t_encode = time.time()

    # Batch inference
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

    t_generate = time.time()

    # Decode results
    results = []
    for i, output in enumerate(outputs):
        input_len = inputs['input_ids'][i].ne(tokenizer.pad_token_id).sum().item()
        generated_ids = output[input_len:]
        text = tokenizer.decode(generated_ids, skip_special_tokens=True)

        results.append({
            "text": text,
            "tokens_generated": len(generated_ids)
        })

    t_end = time.time()

    # è¨ˆç®—æŒ‡æ¨™
    total_tokens = sum(r["tokens_generated"] for r in results)
    throughput = total_tokens / (t_end - t_start)

    metrics = {
        "batch_size": len(prompts),
        "total_tokens": total_tokens,
        "encode_time_ms": round((t_encode - t_start) * 1000, 2),
        "generate_time_ms": round((t_generate - t_encode) * 1000, 2),
        "decode_time_ms": round((t_end - t_generate) * 1000, 2),
        "total_time_ms": round((t_end - t_start) * 1000, 2),
        "throughput_tok_per_s": round(throughput, 2),
        "avg_latency_ms": round((t_end - t_start) * 1000 / len(prompts), 2)
    }

    return results, metrics

# ===== API ç«¯é» =====
@app.post("/generate_batch")
async def generate_batch(req: BatchRequest):
    """æ‰¹æ¬¡ç”Ÿæˆï¼ˆæ¸¬è©¦ç”¨ï¼‰"""
    try:
        results, metrics = batch_generate(
            req.prompts,
            req.max_new_tokens,
            req.temperature
        )
        return {
            "results": results,
            "metrics": metrics
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/generate_single")
async def generate_single(req: GenRequest):
    """å–®ä¸€è«‹æ±‚ï¼ˆå°ç…§çµ„ï¼‰"""
    try:
        results, metrics = batch_generate(
            [req.prompt],
            req.max_new_tokens,
            req.temperature
        )
        return {
            "text": results[0]["text"],
            "metrics": metrics
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    return {
        "status": "ok",
        "model": MODEL_ID,
        "device": DEVICE
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)
'''

with open('batch_server.py', 'w') as f:
    f.write(server_code)

print("\nâœ… batch_server.py å·²ç”Ÿæˆ")

# ============================================================
# PART 3: å•Ÿå‹• Server
# ============================================================

import subprocess
import time
import requests
import signal
import os

print("\nğŸš€ å•Ÿå‹• Server...")
server_proc = subprocess.Popen(
    [sys.executable, "-m", "uvicorn", "batch_server:app",
     "--host", "0.0.0.0", "--port", "8002"],
    stdout=subprocess.PIPE,
    stderr=subprocess.STDOUT,
    text=True
)

# ç­‰å¾… server å•Ÿå‹•
print("â³ ç­‰å¾… server å•Ÿå‹•...")
max_retries = 30
for i in range(max_retries):
    try:
        resp = requests.get("http://localhost:8002/health", timeout=2)
        if resp.status_code == 200:
            print(f"âœ… Server å°±ç·’: {resp.json()}")
            break
    except:
        pass

    if i == max_retries - 1:
        print("âŒ Server å•Ÿå‹•é€¾æ™‚")
        server_proc.terminate()
        raise RuntimeError("Server å•Ÿå‹•å¤±æ•—")

    time.sleep(1)

# ============================================================
# PART 4: å¯¦é©—ï¼šBatch Size å° Throughput çš„å½±éŸ¿
# ============================================================

print("\n" + "="*60)
print("ğŸ“Š å¯¦é©—é–‹å§‹ï¼šæ¸¬è©¦ä¸åŒ Batch Size")
print("="*60)

import pandas as pd

# æ¸¬è©¦ç”¨ prompts
test_prompts = [
    "Explain machine learning in simple terms.",
    "What is photosynthesis and why is it important?",
    "Describe how neural networks work.",
    "How do quantum computers differ from classical computers?",
    "What causes climate change?",
    "Explain the theory of relativity.",
    "How does the human brain process information?",
    "What is the difference between DNA and RNA?",
]

results = []

# æ¸¬è©¦ä¸åŒ batch sizes
batch_sizes = [1, 2, 4, 8]

for batch_size in batch_sizes:
    print(f"\nğŸ”„ æ¸¬è©¦ Batch Size = {batch_size}")

    # æ¯å€‹ batch size æ¸¬è©¦ 5 æ¬¡
    for trial in range(5):
        # æº–å‚™ prompts
        prompts = test_prompts[:batch_size]

        try:
            resp = requests.post(
                "http://localhost:8002/generate_batch",
                json={
                    "prompts": prompts,
                    "max_new_tokens": 64,
                    "temperature": 0.7
                },
                timeout=120
            )

            data = resp.json()
            metrics = data["metrics"]
            metrics["trial"] = trial + 1
            results.append(metrics)

            print(f"  Trial {trial+1}: "
                  f"{metrics['throughput_tok_per_s']:.2f} tok/s, "
                  f"{metrics['avg_latency_ms']:.2f} ms/req")

        except Exception as e:
            print(f"  âŒ Trial {trial+1} å¤±æ•—: {e}")

    time.sleep(1)  # é¿å…éç†±

# å„²å­˜çµæœ
df = pd.DataFrame(results)
df.to_csv('poc2_batch_results.csv', index=False)
print(f"\nâœ… å¯¦é©—å®Œæˆï¼å…± {len(df)} ç­†æ•¸æ“š")
print(f"ğŸ“ çµæœå·²å„²å­˜è‡³ poc2_batch_results.csv")

# ============================================================
# PART 5: ç”Ÿæˆåœ–è¡¨
# ============================================================

print("\nğŸ“ˆ ç”Ÿæˆåˆ†æåœ–è¡¨...")

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

sns.set_style("whitegrid")
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']

# è¨ˆç®—çµ±è¨ˆæ•¸æ“š
stats = df.groupby('batch_size').agg({
    'throughput_tok_per_s': ['mean', 'std'],
    'avg_latency_ms': ['mean', 'std'],
    'total_tokens': 'mean'
}).reset_index()

stats.columns = ['batch_size', 'throughput_mean', 'throughput_std',
                 'latency_mean', 'latency_std', 'tokens_mean']

# åœ– 1: Throughput vs Batch Size
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Throughput
ax1.bar(stats['batch_size'], stats['throughput_mean'],
        yerr=stats['throughput_std'], capsize=5,
        color=['#3b82f6', '#10b981', '#f59e0b', '#ef4444'],
        alpha=0.8, edgecolor='black', linewidth=1.5)

ax1.set_xlabel('Batch Size', fontsize=13, fontweight='bold')
ax1.set_ylabel('Throughput (tokens/sec)', fontsize=13, fontweight='bold')
ax1.set_title('PoC#2: Impact of Batch Size on Throughput', fontsize=15, fontweight='bold')
ax1.set_xticks(stats['batch_size'])
ax1.grid(axis='y', alpha=0.3)

# æ¨™è¨»æ•¸å€¼
for i, row in stats.iterrows():
    ax1.text(row['batch_size'], row['throughput_mean'] + row['throughput_std'] + 5,
             f"{row['throughput_mean']:.1f}", ha='center', fontsize=11, fontweight='bold')

# Latency
ax2.bar(stats['batch_size'], stats['latency_mean'],
        yerr=stats['latency_std'], capsize=5,
        color=['#3b82f6', '#10b981', '#f59e0b', '#ef4444'],
        alpha=0.8, edgecolor='black', linewidth=1.5)

ax2.set_xlabel('Batch Size', fontsize=13, fontweight='bold')
ax2.set_ylabel('Avg Latency per Request (ms)', fontsize=13, fontweight='bold')
ax2.set_title('PoC#2: Impact of Batch Size on Latency', fontsize=15, fontweight='bold')
ax2.set_xticks(stats['batch_size'])
ax2.grid(axis='y', alpha=0.3)

# æ¨™è¨»æ•¸å€¼
for i, row in stats.iterrows():
    ax2.text(row['batch_size'], row['latency_mean'] + row['latency_std'] + 20,
             f"{row['latency_mean']:.1f}", ha='center', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.savefig('poc2_throughput_comparison.png', dpi=300, bbox_inches='tight')
print("âœ… poc2_throughput_comparison.png å·²ç”Ÿæˆ")

# åœ– 2: æ•ˆèƒ½æå‡å€ç‡
fig, ax = plt.subplots(figsize=(10, 6))

baseline = stats[stats['batch_size'] == 1]['throughput_mean'].values[0]
speedup = stats['throughput_mean'] / baseline

bars = ax.bar(stats['batch_size'], speedup,
              color=['#94a3b8', '#10b981', '#f59e0b', '#ef4444'],
              alpha=0.8, edgecolor='black', linewidth=1.5)

ax.axhline(y=1, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Baseline (Batch=1)')
ax.set_xlabel('Batch Size', fontsize=13, fontweight='bold')
ax.set_ylabel('Throughput Speedup (relative to Batch=1)', fontsize=13, fontweight='bold')
ax.set_title('PoC#2: Batch Processing Speedup', fontsize=15, fontweight='bold')
ax.set_xticks(stats['batch_size'])
ax.legend(fontsize=11)
ax.grid(axis='y', alpha=0.3)

# æ¨™è¨»å€ç‡
for i, (bs, sp) in enumerate(zip(stats['batch_size'], speedup)):
    ax.text(bs, sp + 0.1, f"{sp:.2f}x", ha='center', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('poc2_speedup_chart.png', dpi=300, bbox_inches='tight')
print("âœ… poc2_speedup_chart.png å·²ç”Ÿæˆ")

# ============================================================
# PART 6: æ‘˜è¦å ±å‘Š
# ============================================================

print("\n" + "="*60)
print("ğŸ“Š PoC#2 å¯¦é©—çµæœåˆ†æ")
print("="*60)

for _, row in stats.iterrows():
    bs = int(row['batch_size'])
    tput = row['throughput_mean']
    lat = row['latency_mean']

    if bs == 1:
        baseline_tput = tput
        baseline_lat = lat

    speedup = tput / baseline_tput if bs > 1 else 1.0
    lat_increase = ((lat - baseline_lat) / baseline_lat * 100) if bs > 1 else 0

    print(f"\nğŸ“ˆ Batch Size = {bs}:")
    print(f"  â”œâ”€ ååé‡: {tput:.2f} Â± {row['throughput_std']:.2f} tok/s")
    if bs > 1:
        print(f"  â”œâ”€ æå‡å€ç‡: {speedup:.2f}x (ç›¸å°æ–¼ Batch=1)")
    print(f"  â”œâ”€ å¹³å‡å»¶é²: {lat:.2f} Â± {row['latency_std']:.2f} ms/req")
    if bs > 1:
        print(f"  â””â”€ å»¶é²å¢åŠ : {lat_increase:.1f}%")

# æœ€ä½³é…ç½®å»ºè­°
best_throughput_idx = stats['throughput_mean'].idxmax()
best_bs = int(stats.loc[best_throughput_idx, 'batch_size'])

print(f"\nâœ… é—œéµç™¼ç¾:")
print(f"  1. æœ€é«˜ååé‡: Batch Size = {best_bs}")
print(f"  2. Batch Size å¾ 1â†’8: ååé‡æå‡ {speedup:.1f}x")
print(f"  3. å»¶é²æ¬Šè¡¡: Batch è¶Šå¤§ï¼Œå–®æ¬¡è«‹æ±‚å»¶é²å¢åŠ ")
print(f"  4. GPU åˆ©ç”¨ç‡: Batch è¶Šå¤§è¶Šå¥½ï¼ˆæ¸›å°‘è¨˜æ†¶é«”é–’ç½®ï¼‰")

print("\nğŸ’¡ çµè«–:")
print("  â€¢ å°æ–¼é«˜ååé‡å ´æ™¯ â†’ ä½¿ç”¨è¼ƒå¤§ Batch Size (4-8)")
print("  â€¢ å°æ–¼ä½å»¶é²éœ€æ±‚ â†’ ä½¿ç”¨å° Batch Size (1-2)")
print("  â€¢ ç”Ÿç”¢ç’°å¢ƒå»ºè­° â†’ å‹•æ…‹èª¿æ•´ Batch Sizeï¼ˆå¦‚ vLLMï¼‰")

print("\nğŸ“ ç”¢å‡ºæª”æ¡ˆ:")
print("  - poc2_batch_results.csv (åŸå§‹æ•¸æ“š)")
print("  - poc2_throughput_comparison.png (ååé‡èˆ‡å»¶é²å°æ¯”)")
print("  - poc2_speedup_chart.png (åŠ é€Ÿå€ç‡)")

# ============================================================
# PART 7: æ¸…ç†è³‡æº
# ============================================================

print("\nğŸ§¹ æ¸…ç†è³‡æº...")
try:
    server_proc.terminate()
    server_proc.wait(timeout=5)
    print("âœ… Server å·²é—œé–‰")
except:
    server_proc.kill()
    print("âš ï¸  å¼·åˆ¶çµ‚æ­¢ Server")

print("\nğŸ‰ PoC#2 å®Œæˆï¼")
print("="*60)